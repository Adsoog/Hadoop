wget https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop2.7.tgz
tar -xvf spark-3.2.4-bin-hadoop2.7.tgz
vi ~/.bashrc
export SPARK_HOME=spark-3.2.4-bin-hadoop2.7
export PATH=$PATH:$SPARK_HOME/bin
source ~/.bashrc

///////////////////////
# .bashrc

# User specific aliases and functions
alias rm='rm -i'
alias cp='cp -i'
alias mv='mv -i'

# Spark variables de entorno
export SPARK_HOME=spark-3.2.4-bin-hadoop2.7
export PATH=$PATH:$SPARK_HOME/bin

# Source global definitions
if [ -f /etc/bashrc ]; then
        . /etc/bashrc
fi
export SPARK_MAJOR_VERSION=2
systemctl start systemd-user-sessions.service

////////////////////////////////

spark-submit 

sudo yum remove python36u
sudo yum remove python3
sudo yum install python3
python3 --version

vi prueba2.py

////////////////////////////////////////////
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Crear una instancia de SparkSession
spark = SparkSession.builder.appName("MiAppSpark").getOrCreate()

# Definir el esquema de la tabla
schema = StructType([
    StructField("nombre", StringType(), nullable=False),
    StructField("edad", IntegerType(), nullable=False)
])

# Datos de ejemplo
data = [
    ("Juan", 25),
    ("María", 30),
    ("Pedro", 40),
    ("Ana", 35),
    ("Luis", 27)
]

# Crear un DataFrame a partir de los datos y el esquema
df = spark.createDataFrame(data, schema)

# Mostrar el contenido del DataFrame
df.show()

# Cerrar la sesión de Spark al finalizar
spark.stop()
/////////////////////////////////////////////////////////

spark-submit --master local[*] prueba2.py




